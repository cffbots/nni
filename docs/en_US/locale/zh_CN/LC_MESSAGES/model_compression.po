# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, Microsoft
# This file is distributed under the same license as the NNI package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: NNI \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-01-29 17:40+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../model_compression.rst:23
msgid "Overview"
msgstr ""

#: ../../model_compression.rst:23
msgid "Quick Start"
msgstr ""

#: ../../model_compression.rst:23
msgid "Tutorial"
msgstr ""

#: ../../model_compression.rst:23
msgid "Pruning"
msgstr ""

#: ../../model_compression.rst:23
msgid "Pruning V2"
msgstr ""

#: ../../model_compression.rst:23
msgid "Quantization"
msgstr ""

#: ../../model_compression.rst:23
msgid "Utilities"
msgstr ""

#: ../../model_compression.rst:23
msgid "Advanced Usage"
msgstr ""

#: ../../model_compression.rst:23
msgid "API Reference"
msgstr ""

#: ../../model_compression.rst:3
msgid "Model Compression"
msgstr ""

#: ../../model_compression.rst:5
msgid ""
"Deep neural networks (DNNs) have achieved great success in many tasks. "
"However, typical neural networks are both computationally expensive and "
"energy intensive, can be difficult to be deployed on devices with low "
"computation resources or with strict latency requirements. Therefore, a "
"natural thought is to perform model compression to reduce model size and "
"accelerate model training/inference without losing performance "
"significantly. Model compression techniques can be divided into two "
"categories: pruning and quantization. The pruning methods explore the "
"redundancy in the model weights and try to remove/prune the redundant and"
" uncritical weights. Quantization refers to compressing models by "
"reducing the number of bits required to represent weights or activations."
msgstr ""

#: ../../model_compression.rst:13
msgid ""
"NNI provides an easy-to-use toolkit to help user design and use model "
"pruning and quantization algorithms. It supports Tensorflow and PyTorch "
"with unified interface. For users to compress their models, they only "
"need to add several lines in their code. There are some popular model "
"compression algorithms built-in in NNI. Users could further use NNI's "
"auto tuning power to find the best compressed model, which is detailed in"
" Auto Model Compression. On the other hand, users could easily customize "
"their new compression algorithms using NNI's interface."
msgstr ""

#: ../../model_compression.rst:21
msgid "For details, please refer to the following tutorials:"
msgstr ""

