# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, Microsoft
# This file is distributed under the same license as the NNI package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: NNI \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-01-29 17:40+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../hpo_benchmark_stats.rst:2
msgid "HPO Benchmark Example Statistics"
msgstr ""

#: ../../hpo_benchmark_stats.rst:5
msgid "A Benchmark Example"
msgstr ""

#: ../../hpo_benchmark_stats.rst:7
msgid ""
"As an example, we ran the \"nnismall\" benchmark with the random forest "
"search space on the following 8 tuners: \"TPE\", \"Random\", \"Anneal\", "
"\"Evolution\", \"SMAC\", \"GPTuner\", \"MetisTuner\", \"DNGOTuner\". For "
"convenience of reference, we also list the search space we experimented "
"on here. Note that the way in which the search space is written may "
"significantly affect hyperparameter optimization performance, and we plan"
" to conduct further experiments on how well NNI built-in tuners adapt to "
"different search space formulations using this benchmarking tool."
msgstr ""

#: ../../hpo_benchmark_stats.rst:23
msgid ""
"As some of the tasks contains a considerable amount of training data, it "
"took about 2 days to run the whole benchmark on one tuner. For a more "
"detailed description of the tasks, please check "
"``/examples/trials/benchmarking/automlbenchmark/nni/benchmarks/nnismall_description.txt``."
" For binary and multi-class classification tasks, the metric \"auc\" and "
"\"logloss\" were used for evaluation, while for regression, \"r2\" and "
"\"rmse\" were used."
msgstr ""

#: ../../hpo_benchmark_stats.rst:28
msgid ""
"After the script finishes, the final scores of each tuner are summarized "
"in the file ``results[time]/reports/performances.txt``. Since the file is"
" large, we only show the following screenshot and summarize other "
"important statistics instead."
msgstr ""

#: ../../hpo_benchmark_stats.rst:35
msgid ""
"When the results are parsed, the tuners are also ranked based on their "
"final performance. The following three tables show the average ranking of"
" the tuners for each metric (logloss, rmse, auc)."
msgstr ""

#: ../../hpo_benchmark_stats.rst:38
msgid ""
"Also, for every tuner, their performance for each type of metric is "
"summarized (another view of the same data). We present this statistics in"
" the fourth table. Note that this information can be found at "
"``results[time]/reports/rankings.txt``."
msgstr ""

#: ../../hpo_benchmark_stats.rst:41
msgid ""
"Average rankings for metric rmse (for regression tasks). We found that "
"Anneal performs the best among all NNI built-in tuners."
msgstr ""

#: ../../hpo_benchmark_stats.rst:46 ../../hpo_benchmark_stats.rst:70
#: ../../hpo_benchmark_stats.rst:94 ../../hpo_benchmark_stats.rst:118
msgid "Tuner Name"
msgstr ""

#: ../../hpo_benchmark_stats.rst:47 ../../hpo_benchmark_stats.rst:71
#: ../../hpo_benchmark_stats.rst:95
msgid "Average Ranking"
msgstr ""

#: ../../hpo_benchmark_stats.rst:48 ../../hpo_benchmark_stats.rst:78
#: ../../hpo_benchmark_stats.rst:106 ../../hpo_benchmark_stats.rst:130
msgid "Anneal"
msgstr ""

#: ../../hpo_benchmark_stats.rst:49 ../../hpo_benchmark_stats.rst:131
msgid "3.75"
msgstr ""

#: ../../hpo_benchmark_stats.rst:50 ../../hpo_benchmark_stats.rst:84
#: ../../hpo_benchmark_stats.rst:96 ../../hpo_benchmark_stats.rst:126
msgid "Random"
msgstr ""

#: ../../hpo_benchmark_stats.rst:51 ../../hpo_benchmark_stats.rst:75
#: ../../hpo_benchmark_stats.rst:127 ../../hpo_benchmark_stats.rst:140
msgid "4.00"
msgstr ""

#: ../../hpo_benchmark_stats.rst:52 ../../hpo_benchmark_stats.rst:76
#: ../../hpo_benchmark_stats.rst:108 ../../hpo_benchmark_stats.rst:134
msgid "Evolution"
msgstr ""

#: ../../hpo_benchmark_stats.rst:53 ../../hpo_benchmark_stats.rst:55
#: ../../hpo_benchmark_stats.rst:135 ../../hpo_benchmark_stats.rst:151
msgid "4.44"
msgstr ""

#: ../../hpo_benchmark_stats.rst:54 ../../hpo_benchmark_stats.rst:86
#: ../../hpo_benchmark_stats.rst:98 ../../hpo_benchmark_stats.rst:150
msgid "DNGOTuner"
msgstr ""

#: ../../hpo_benchmark_stats.rst:56 ../../hpo_benchmark_stats.rst:72
#: ../../hpo_benchmark_stats.rst:100 ../../hpo_benchmark_stats.rst:146
msgid "SMAC"
msgstr ""

#: ../../hpo_benchmark_stats.rst:57 ../../hpo_benchmark_stats.rst:147
msgid "4.56"
msgstr ""

#: ../../hpo_benchmark_stats.rst:58 ../../hpo_benchmark_stats.rst:82
#: ../../hpo_benchmark_stats.rst:104 ../../hpo_benchmark_stats.rst:122
msgid "TPE"
msgstr ""

#: ../../hpo_benchmark_stats.rst:59 ../../hpo_benchmark_stats.rst:61
#: ../../hpo_benchmark_stats.rst:63 ../../hpo_benchmark_stats.rst:123
#: ../../hpo_benchmark_stats.rst:139 ../../hpo_benchmark_stats.rst:143
msgid "4.94"
msgstr ""

#: ../../hpo_benchmark_stats.rst:60 ../../hpo_benchmark_stats.rst:74
#: ../../hpo_benchmark_stats.rst:102 ../../hpo_benchmark_stats.rst:138
msgid "GPTuner"
msgstr ""

#: ../../hpo_benchmark_stats.rst:62 ../../hpo_benchmark_stats.rst:80
#: ../../hpo_benchmark_stats.rst:110 ../../hpo_benchmark_stats.rst:142
msgid "MetisTuner"
msgstr ""

#: ../../hpo_benchmark_stats.rst:65
msgid ""
"Average rankings for metric auc (for classification tasks). We found that"
" SMAC performs the best among all NNI built-in tuners."
msgstr ""

#: ../../hpo_benchmark_stats.rst:73 ../../hpo_benchmark_stats.rst:148
msgid "3.67"
msgstr ""

#: ../../hpo_benchmark_stats.rst:77 ../../hpo_benchmark_stats.rst:136
msgid "4.22"
msgstr ""

#: ../../hpo_benchmark_stats.rst:79 ../../hpo_benchmark_stats.rst:81
#: ../../hpo_benchmark_stats.rst:132 ../../hpo_benchmark_stats.rst:144
msgid "4.39"
msgstr ""

#: ../../hpo_benchmark_stats.rst:83 ../../hpo_benchmark_stats.rst:124
msgid "4.67"
msgstr ""

#: ../../hpo_benchmark_stats.rst:85 ../../hpo_benchmark_stats.rst:87
#: ../../hpo_benchmark_stats.rst:128 ../../hpo_benchmark_stats.rst:152
msgid "5.33"
msgstr ""

#: ../../hpo_benchmark_stats.rst:89
msgid ""
"Average rankings for metric logloss (for classification tasks). We found "
"that Random performs the best among all NNI built-in tuners."
msgstr ""

#: ../../hpo_benchmark_stats.rst:97 ../../hpo_benchmark_stats.rst:129
msgid "3.36"
msgstr ""

#: ../../hpo_benchmark_stats.rst:99 ../../hpo_benchmark_stats.rst:153
msgid "3.50"
msgstr ""

#: ../../hpo_benchmark_stats.rst:101 ../../hpo_benchmark_stats.rst:149
msgid "3.93"
msgstr ""

#: ../../hpo_benchmark_stats.rst:103 ../../hpo_benchmark_stats.rst:141
msgid "4.64"
msgstr ""

#: ../../hpo_benchmark_stats.rst:105 ../../hpo_benchmark_stats.rst:125
msgid "4.71"
msgstr ""

#: ../../hpo_benchmark_stats.rst:107 ../../hpo_benchmark_stats.rst:133
msgid "4.93"
msgstr ""

#: ../../hpo_benchmark_stats.rst:109 ../../hpo_benchmark_stats.rst:137
msgid "5.00"
msgstr ""

#: ../../hpo_benchmark_stats.rst:111 ../../hpo_benchmark_stats.rst:145
msgid "5.93"
msgstr ""

#: ../../hpo_benchmark_stats.rst:113
msgid ""
"To view the same data in another way, for each tuner, we present the "
"average rankings on different types of metrics. From the table, we can "
"find that, for example, the DNGOTuner performs better for the tasks whose"
" metric is \"logloss\" than for the tasks with metric \"auc\". We hope "
"this information can to some extent guide the choice of tuners given some"
" knowledge of task types."
msgstr ""

#: ../../hpo_benchmark_stats.rst:119
msgid "rmse"
msgstr ""

#: ../../hpo_benchmark_stats.rst:120
msgid "auc"
msgstr ""

#: ../../hpo_benchmark_stats.rst:121
msgid "logloss"
msgstr ""

#: ../../hpo_benchmark_stats.rst:155
msgid ""
"Besides these reports, our script also generates two graphs for each fold"
" of each task: one graph presents the best score received by each tuner "
"until trial x, and another graph shows the score that each tuner receives"
" in trial x. These two graphs can give some information regarding how the"
" tuners are \"converging\" to their final solution. We found that for "
"\"nnismall\", tuners on the random forest model with search space defined"
" in "
"``/examples/trials/benchmarking/automlbenchmark/nni/extensions/NNI/architectures/run_random_forest.py``"
" generally converge to the final solution after 40 to 60 trials. As there"
" are too much graphs to incldue in a single report (96 graphs in total), "
"we only present 10 graphs here."
msgstr ""

#: ../../hpo_benchmark_stats.rst:166
msgid ""
"The previous two graphs are generated for fold 1 of the task \"car\". In "
"the first graph, we observe that most tuners find a relatively good "
"solution within 40 trials. In this experiment, among all tuners, the "
"DNGOTuner converges fastest to the best solution (within 10 trials). Its "
"best score improved for three times in the entire experiment. In the "
"second graph, we observe that most tuners have their score flucturate "
"between 0.8 and 1 throughout the experiment. However, it seems that the "
"Anneal tuner (green line) is more unstable (having more fluctuations) "
"while the GPTuner has a more stable pattern. This may be interpreted as "
"the Anneal tuner explores more aggressively than the GPTuner and thus its"
" scores for different trials vary a lot. Regardless, although this "
"pattern can to some extent hint a tuner's position on the explore-exploit"
" tradeoff, it is not a comprehensive evaluation of a tuner's "
"effectiveness."
msgstr ""

