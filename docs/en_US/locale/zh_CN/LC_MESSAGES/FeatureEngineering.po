# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, Microsoft
# This file is distributed under the same license as the NNI package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: NNI \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-01-29 17:40+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../FeatureEngineering/GBDTSelector.rst:2
msgid "GBDTSelector"
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:4
msgid ""
"GBDTSelector is based on `LightGBM "
"<https://github.com/microsoft/LightGBM>`__\\ , which is a gradient "
"boosting framework that uses tree-based learning algorithms."
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:6
msgid ""
"When passing the data into the GBDT model, the model will construct the "
"boosting tree. And the feature importance comes from the score in "
"construction, which indicates how useful or valuable each feature was in "
"the construction of the boosted decision trees within the model."
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:8
msgid ""
"We could use this method as a strong baseline in Feature Selector, "
"especially when using the GBDT model as a classifier or regressor."
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:10
msgid ""
"For now, we support the ``importance_type`` is ``split`` and ``gain``. "
"But we will support customized ``importance_type`` in the future, which "
"means the user could define how to calculate the ``feature score`` by "
"themselves."
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:13
#: ../../FeatureEngineering/GradientFeatureSelector.rst:17
msgid "Usage"
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:15
msgid "First you need to install dependency:"
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:21
msgid "Then"
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:41
msgid ""
"And you could reference the examples in "
"``/examples/feature_engineering/gbdt_selector/``\\ , too."
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:43
#: ../../FeatureEngineering/GradientFeatureSelector.rst:93
msgid "**Requirement of fit FuncArgs**"
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:47
msgid ""
"**X** (array-like, require) - The training input samples which shape = "
"[n_samples, n_features]"
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:50
msgid ""
"**y** (array-like, require) - The target values (class labels in "
"classification, real numbers in regression) which shape = [n_samples]."
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:53
msgid ""
"**lgb_params** (dict, require) - The parameters for lightgbm model. The "
"detail you could reference `here "
"<https://lightgbm.readthedocs.io/en/latest/Parameters.html>`__"
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:56
msgid ""
"**eval_ratio** (float, require) - The ratio of data size. It's used for "
"split the eval data and train data from self.X."
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:59
msgid ""
"**early_stopping_rounds** (int, require) - The early stopping setting in "
"lightgbm. The detail you could reference `here "
"<https://lightgbm.readthedocs.io/en/latest/Parameters.html>`__."
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:62
msgid ""
"**importance_type** (str, require) - could be 'split' or 'gain'. The "
"'split' means ' result contains numbers of times the feature is used in a"
" model' and the 'gain' means 'result contains total gains of splits which"
" use the feature'. The detail you could reference in `here "
"<https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster.feature_importance>`__."
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:65
msgid ""
"**num_boost_round** (int, require) - number of boost round. The detail "
"you could reference `here "
"<https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.train.html#lightgbm.train>`__."
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:67
#: ../../FeatureEngineering/GradientFeatureSelector.rst:105
msgid "**Requirement of get_selected_features FuncArgs**"
msgstr ""

#: ../../FeatureEngineering/GBDTSelector.rst:70
msgid ""
"**topk** (int, require) - the topK impotance features you want to "
"selected."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:2
msgid "GradientFeatureSelector"
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:4
msgid ""
"The algorithm in GradientFeatureSelector comes from `Feature Gradients: "
"Scalable Feature Selection via Discrete Relaxation "
"<https://arxiv.org/pdf/1908.10382.pdf>`__."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:6
msgid ""
"GradientFeatureSelector, a gradient-based search algorithm for feature "
"selection."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:9
msgid ""
"1) This approach extends a recent result on the estimation of "
"learnability in the sublinear data regime by showing that the calculation"
" can be performed iteratively (i.e., in mini-batches) and in **linear "
"time and space** with respect to both the number of features D and the "
"sample size N."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:12
msgid ""
"This, along with a discrete-to-continuous relaxation of the search "
"domain, allows for an **efficient, gradient-based** search algorithm "
"among feature subsets for very **large datasets**."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:14
msgid ""
"Crucially, this algorithm is capable of finding **higher-order "
"correlations** between features and targets for both the N > D and N < D "
"regimes, as opposed to approaches that do not consider such interactions "
"and/or only consider one regime."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:37
msgid ""
"And you could reference the examples in "
"``/examples/feature_engineering/gradient_feature_selector/``\\ , too."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:39
msgid "**Parameters of class FeatureGradientSelector constructor**"
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:43
msgid ""
"**order** (int, optional, default = 4) - What order of interactions to "
"include. Higher orders may be more accurate but increase the run time. 12"
" is the maximum allowed order."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:46
msgid ""
"**penatly** (int, optional, default = 1) - Constant that multiplies the "
"regularization term."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:49
msgid ""
"**n_features** (int, optional, default = None) - If None, will "
"automatically choose number of features based on search. Otherwise, the "
"number of top features to select."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:52
msgid ""
"**max_features** (int, optional, default = None) - If not None, will use "
"the 'elbow method' to determine the number of features with max_features "
"as the upper limit."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:55
msgid "**learning_rate** (float, optional, default = 1e-1) - learning rate"
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:58
msgid ""
"**init** (*zero, on, off, onhigh, offhigh, or sklearn, optional, default "
"= zero*\\ ) - How to initialize the vector of scores. 'zero' is the "
"default."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:61
msgid "**n_epochs** (int, optional, default = 1) - number of epochs to run"
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:64
msgid ""
"**shuffle** (bool, optional, default = True) - Shuffle \"rows\" prior to "
"an epoch."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:67
msgid ""
"**batch_size** (int, optional, default = 1000) - Nnumber of \"rows\" to "
"process at a time."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:70
msgid ""
"**target_batch_size** (int, optional, default = 1000) - Number of "
"\"rows\" to accumulate gradients over. Useful when many rows will not fit"
" into memory but are needed for accurate estimation."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:73
msgid ""
"**classification** (bool, optional, default = True) - If True, problem is"
" classification, else regression."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:76
msgid ""
"**ordinal** (bool, optional, default = True) - If True, problem is "
"ordinal classification. Requires classification to be True."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:79
msgid ""
"**balanced** (bool, optional, default = True) - If true, each class is "
"weighted equally in optimization, otherwise weighted is done via support "
"of each class. Requires classification to be True."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:82
msgid ""
"**prerocess** (str, optional, default = 'zscore') - 'zscore' which refers"
" to centering and normalizing data to unit variance or 'center' which "
"only centers the data to 0 mean."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:85
msgid ""
"**soft_grouping** (bool, optional, default = True) - If True, groups "
"represent features that come from the same source. Used to encourage "
"sparsity of groups and features within groups."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:88
msgid ""
"**verbose** (int, optional, default = 0) - Controls the verbosity when "
"fitting. Set to 0 for no printing 1 or higher for printing every verbose "
"number of gradient steps."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:91
msgid ""
"**device** (str, optional, default = 'cpu') - 'cpu' to run on CPU and "
"'cuda' to run on GPU. Runs much faster on GPU"
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:97
msgid ""
"**X** (array-like, require) - The training input samples which shape = "
"[n_samples, n_features]. `np.ndarry` recommended."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:100
msgid ""
"**y** (array-like, require) - The target values (class labels in "
"classification, real numbers in regression) which shape = [n_samples]. "
"`np.ndarry` recommended."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:103
msgid ""
"**groups** (array-like, optional, default = None) - Groups of columns "
"that must be selected as a unit. e.g. [0, 0, 1, 2] specifies the first "
"two columns are part of a group. Which shape is [n_features]."
msgstr ""

#: ../../FeatureEngineering/GradientFeatureSelector.rst:107
msgid "For now, the ``get_selected_features`` function has no parameters."
msgstr ""

#: ../../FeatureEngineering/Overview.rst:2
msgid "Feature Engineering with NNI"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:4
msgid ""
"We are glad to announce the alpha release for Feature Engineering toolkit"
" on top of NNI, it's still in the experiment phase which might evolve "
"based on user feedback. We'd like to invite you to use, feedback and even"
" contribute."
msgstr ""

#: ../../FeatureEngineering/Overview.rst:6
msgid "For now, we support the following feature selector:"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:9
msgid "`GradientFeatureSelector <./GradientFeatureSelector.rst>`__"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:10
msgid "`GBDTSelector <./GBDTSelector.rst>`__"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:12
msgid ""
"These selectors are suitable for tabular data(which means it doesn't "
"include image, speech and text data)."
msgstr ""

#: ../../FeatureEngineering/Overview.rst:14
msgid ""
"In addition, those selector only for feature selection. If you want to: "
"1) generate high-order combined features on nni while doing feature "
"selection; 2) leverage your distributed resources; you could try this "
":githublink:`example <examples/feature_engineering/auto-feature-"
"engineering>`."
msgstr ""

#: ../../FeatureEngineering/Overview.rst:20
msgid "How to use?"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:41
msgid ""
"When using the built-in Selector, you first need to ``import`` a feature "
"selector, and ``initialize`` it. You could call the function ``fit`` in "
"the selector to pass the data to the selector. After that, you could use "
"``get_seleteced_features`` to get important features. The function "
"parameters in different selectors might be different, so you need to "
"check the docs before using it."
msgstr ""

#: ../../FeatureEngineering/Overview.rst:44
msgid "How to customize?"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:46
msgid ""
"NNI provides *state-of-the-art* feature selector algorithm in the "
"builtin-selector. NNI also supports to build a feature selector by "
"yourself."
msgstr ""

#: ../../FeatureEngineering/Overview.rst:48
msgid "If you want to implement a customized feature selector, you need to:"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:51
msgid "Inherit the base FeatureSelector class"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:52
msgid "Implement *fit* and _get_selected *features* function"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:53
msgid "Integrate with sklearn (Optional)"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:55
#: ../../FeatureEngineering/Overview.rst:119
msgid "Here is an example:"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:57
msgid "**1. Inherit the base Featureselector Class**"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:67
msgid "**2. Implement fit and _get_selected features Function**"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:108
msgid "**3. Integrate with Sklearn**"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:110
msgid ""
"``sklearn.pipeline.Pipeline`` can connect models in series, such as "
"feature selector, normalization, and classification/regression to form a "
"typical machine learning problem workflow. The following step could help "
"us to better integrate with sklearn, which means we could treat the "
"customized feature selector as a module of the pipeline."
msgstr ""

#: ../../FeatureEngineering/Overview.rst:114
msgid "Inherit the calss *sklearn.base.BaseEstimator*"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:115
msgid "Implement _get\\ *params* and _set*params* function in *BaseEstimator*"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:116
msgid "Inherit the class _sklearn.feature\\ *selection.base.SelectorMixin*"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:117
msgid ""
"Implement _get\\ *support*\\ , *transform* and _inverse*transform* "
"Function in *SelectorMixin*"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:121
msgid "**1. Inherit the BaseEstimator Class and its Function**"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:150
msgid "**2. Inherit the SelectorMixin Class and its Function**"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:237
msgid ""
"After integrating with Sklearn, we could use the feature selector as "
"follows:"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:256
msgid "Benchmark"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:258
#, python-format
msgid ""
"``Baseline`` means without any feature selection, we directly pass the "
"data to LogisticRegression. For this benchmark, we only use 10% data from"
" the train as test data. For the GradientFeatureSelector, we only take "
"the top20 features. The metric is the mean accuracy on the given test "
"data and labels."
msgstr ""

#: ../../FeatureEngineering/Overview.rst:264
msgid "Dataset"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:265
msgid "All Features + LR (acc, time, memory)"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:266
msgid "GradientFeatureSelector + LR (acc, time, memory)"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:267
msgid "TreeBasedClassifier + LR (acc, time, memory)"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:268
msgid "#Train"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:269
msgid "#Feature"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:270
msgid "colon-cancer"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:271
msgid "0.7547, 890ms, 348MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:272
msgid "0.7368, 363ms, 286MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:273
msgid "0.7223, 171ms, 1171 MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:274
msgid "62"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:275
msgid "2,000"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:276
msgid "gisette"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:277
msgid "0.9725, 215ms, 584MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:278
msgid "0.89416, 446ms, 397MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:279
msgid "0.9792, 911ms, 234MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:280
msgid "6,000"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:281
msgid "5,000"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:282
msgid "avazu"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:283
msgid "0.8834, N/A, N/A"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:284
#: ../../FeatureEngineering/Overview.rst:285
msgid "N/A, N/A, N/A"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:286
msgid "40,428,967"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:287
msgid "1,000,000"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:288
msgid "rcv1"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:289
msgid "0.9644, 557ms, 241MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:290
msgid "0.7333, 401ms, 281MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:291
msgid "0.9615, 752ms, 284MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:292
msgid "20,242"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:293
msgid "47,236"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:294
msgid "news20.binary"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:295
msgid "0.9208, 707ms, 361MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:296
msgid "0.6870, 565ms, 371MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:297
msgid "0.9070, 904ms, 364MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:298
msgid "19,996"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:299
msgid "1,355,191"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:300
msgid "real-sim"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:301
msgid "0.9681, 433ms, 274MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:302
msgid "0.7969, 251ms, 274MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:303
msgid "0.9591, 643ms, 367MiB"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:304
msgid "72,309"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:305
msgid "20,958"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:308
msgid ""
"The dataset of benchmark could be download in `here "
"<https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/>`__"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:310
msgid ""
"The code could be refenrence "
"``/examples/feature_engineering/gradient_feature_selector/benchmark_test.py``."
msgstr ""

#: ../../FeatureEngineering/Overview.rst:313
msgid "Reference and Feedback"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:316
msgid ""
"To `report a bug <https://github.com/microsoft/nni/issues/new?template"
"=bug-report.rst>`__ for this feature in GitHub;"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:317
msgid ""
"To `file a feature or improvement request "
"<https://github.com/microsoft/nni/issues/new?template=enhancement.rst>`__"
" for this feature in GitHub;"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:318
msgid ""
"To know more about :githublink:`Neural Architecture Search with NNI "
"<docs/en_US/NAS/Overview.rst>`\\ ;"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:319
msgid ""
"To know more about :githublink:`Model Compression with NNI "
"<docs/en_US/Compression/Overview.rst>`\\ ;"
msgstr ""

#: ../../FeatureEngineering/Overview.rst:320
msgid ""
"To know more about :githublink:`Hyperparameter Tuning with NNI "
"<docs/en_US/Tuner/BuiltinTuner.rst>`\\ ;"
msgstr ""

