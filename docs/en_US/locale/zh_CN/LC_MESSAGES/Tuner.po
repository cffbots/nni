# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, Microsoft
# This file is distributed under the same license as the NNI package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: NNI \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-01-29 17:40+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../Tuner/AnnealTuner.rst:2
msgid "Anneal Tuner"
msgstr ""

#: ../../Tuner/AnnealTuner.rst:4
msgid ""
"This simple annealing algorithm begins by sampling from the prior but "
"tends over time to sample from points closer and closer to the best ones "
"observed. This algorithm is a simple variation on random search that "
"leverages smoothness in the response surface. The annealing rate is not "
"adaptive."
msgstr ""

#: ../../Tuner/AnnealTuner.rst:7 ../../Tuner/BatchTuner.rst:9
#: ../../Tuner/BohbAdvisor.rst:67 ../../Tuner/DngoTuner.rst:5
#: ../../Tuner/EvolutionTuner.rst:7 ../../Tuner/GPTuner.rst:13
#: ../../Tuner/GridsearchTuner.rst:9 ../../Tuner/HyperbandAdvisor.rst:32
#: ../../Tuner/MetisTuner.rst:24 ../../Tuner/NetworkmorphismTuner.rst:9
#: ../../Tuner/PBTTuner.rst:15 ../../Tuner/RandomTuner.rst:8
#: ../../Tuner/SmacTuner.rst:9 ../../Tuner/TpeTuner.rst:24
msgid "Usage"
msgstr ""

#: ../../Tuner/AnnealTuner.rst:10 ../../Tuner/BohbAdvisor.rst:79
#: ../../Tuner/EvolutionTuner.rst:10 ../../Tuner/NetworkmorphismTuner.rst:17
msgid "classArgs Requirements"
msgstr ""

#: ../../Tuner/AnnealTuner.rst:12 ../../Tuner/EvolutionTuner.rst:13
#: ../../Tuner/HyperbandAdvisor.rst:120 ../../Tuner/NetworkmorphismTuner.rst:19
#: ../../Tuner/SmacTuner.rst:23
msgid ""
"**optimize_mode** (*maximize or minimize, optional, default = maximize*) "
"- If 'maximize', the tuner will try to maximize metrics. If 'minimize', "
"the tuner will try to minimize metrics."
msgstr ""

#: ../../Tuner/AnnealTuner.rst:15 ../../Tuner/BatchTuner.rst:12
#: ../../Tuner/DngoTuner.rst:19 ../../Tuner/EvolutionTuner.rst:19
#: ../../Tuner/GPTuner.rst:29 ../../Tuner/GridsearchTuner.rst:14
#: ../../Tuner/HyperbandAdvisor.rst:126 ../../Tuner/MetisTuner.rst:32
#: ../../Tuner/NetworkmorphismTuner.rst:50 ../../Tuner/PBTTuner.rst:68
#: ../../Tuner/RandomTuner.rst:10 ../../Tuner/SmacTuner.rst:27
msgid "Example Configuration"
msgstr ""

#: ../../Tuner/BatchTuner.rst:2
msgid "Batch Tuner"
msgstr ""

#: ../../Tuner/BatchTuner.rst:4
msgid ""
"Batch tuner allows users to simply provide several configurations (i.e., "
"choices of hyper-parameters) for their trial code. After finishing all "
"the configurations, the experiment is done. Batch tuner only supports the"
" type ``choice`` in the `search space spec "
"<../Tutorial/SearchSpaceSpec.rst>`__."
msgstr ""

#: ../../Tuner/BatchTuner.rst:6
msgid ""
"Suggested scenario: If the configurations you want to try have been "
"decided, you can list them in the SearchSpace file (using ``choice``) and"
" run them using the batch tuner."
msgstr ""

#: ../../Tuner/BatchTuner.rst:20
msgid "Note that the search space for BatchTuner should look like:"
msgstr ""

#: ../../Tuner/BatchTuner.rst:37
msgid ""
"The search space file should include the high-level key "
"``combine_params``. The type of params in the search space must be "
"``choice`` and the ``values`` must include all the combined params "
"values."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:2
msgid "BOHB Advisor"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:4
msgid ""
"BOHB is a robust and efficient hyperparameter tuning algorithm mentioned "
"in `this reference paper <https://arxiv.org/abs/1807.01774>`__. BO is an "
"abbreviation for \"Bayesian Optimization\" and HB is an abbreviation for "
"\"Hyperband\"."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:6
msgid ""
"BOHB relies on HB (Hyperband) to determine how many configurations to "
"evaluate with which budget, but it **replaces the random selection of "
"configurations at the beginning of each HB iteration by a model-based "
"search (Bayesian Optimization)**. Once the desired number of "
"configurations for the iteration is reached, the standard successive "
"halving procedure is carried out using these configurations. We keep "
"track of the performance of all function evaluations g(x, b) of "
"configurations x on all budgets b to use as a basis for our models in "
"later iterations."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:8
msgid "Below we divide the introduction of the BOHB process into two parts:"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:11
msgid "HB (Hyperband)"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:13
msgid ""
"We follow Hyperband’s way of choosing the budgets and continue to use "
"SuccessiveHalving. For more details, you can refer to the `Hyperband in "
"NNI <HyperbandAdvisor.rst>`__ and the `reference paper for Hyperband "
"<https://arxiv.org/abs/1603.06560>`__. This procedure is summarized by "
"the pseudocode below."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:22
msgid "BO (Bayesian Optimization)"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:24
msgid ""
"The BO part of BOHB closely resembles TPE with one major difference: we "
"opted for a single multidimensional KDE compared to the hierarchy of one-"
"dimensional KDEs used in TPE in order to better handle interaction "
"effects in the input space."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:26
msgid ""
"Tree Parzen Estimator(TPE): uses a KDE (kernel density estimator) to "
"model the densities."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:34
msgid ""
"To fit useful KDEs, we require a minimum number of data points Nmin; this"
" is set to d + 1 for our experiments, where d is the number of "
"hyperparameters. To build a model as early as possible, we do not wait "
"until Nb = \\|Db\\|, where the number of observations for budget b is "
"large enough to satisfy q · Nb ≥ Nmin. Instead, after initializing with "
"Nmin + 2 random configurations, we choose the"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:42
msgid "best and worst configurations, respectively, to model the two densities."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:44
msgid ""
"Note that we also sample a constant fraction named **random fraction** of"
" the configurations uniformly at random."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:47
msgid "Workflow"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:56
msgid ""
"This image shows the workflow of BOHB. Here we set max_budget = 9, "
"min_budget = 1, eta = 3, others as default. In this case, s_max = 2, so "
"we will continuously run the {s=2, s=1, s=0, s=2, s=1, s=0, ...} cycle. "
"In each stage of SuccessiveHalving (the orange box), we will pick the top"
" 1/eta configurations and run them again with more budget, repeating the "
"SuccessiveHalving stage until the end of this iteration. At the same "
"time, we collect the configurations, budgets and final metrics of each "
"trial and use these to build a multidimensional KDEmodel with the key "
"\"budget\"."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:56
msgid ""
"Multidimensional KDE is used to guide the selection of configurations for"
" the next iteration."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:58
msgid ""
"The sampling procedure (using Multidimensional KDE to guide selection) is"
" summarized by the pseudocode below."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:70 ../../Tuner/DngoTuner.rst:8
#: ../../Tuner/NetworkmorphismTuner.rst:12 ../../Tuner/SmacTuner.rst:12
msgid "Installation"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:72
msgid ""
"BOHB advisor requires the `ConfigSpace "
"<https://github.com/automl/ConfigSpace>`__ package. ConfigSpace can be "
"installed using the following command."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:81 ../../Tuner/BohbAdvisor.rst:119
msgid ""
"**optimize_mode** (*maximize or minimize, optional, default = maximize*) "
"- If 'maximize', tuners will try to maximize metrics. If 'minimize', "
"tuner will try to minimize metrics."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:82 ../../Tuner/BohbAdvisor.rst:120
msgid ""
"**min_budget** (*int, optional, default = 1*) - The smallest budget to "
"assign to a trial job, (budget can be the number of mini-batches or "
"epochs). Needs to be positive."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:83 ../../Tuner/BohbAdvisor.rst:121
msgid ""
"**max_budget** (*int, optional, default = 3*) - The largest budget to "
"assign to a trial job, (budget can be the number of mini-batches or "
"epochs). Needs to be larger than min_budget."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:84 ../../Tuner/BohbAdvisor.rst:122
msgid ""
"**eta** (*int, optional, default = 3*) - In each iteration, a complete "
"run of sequential halving is executed. In it, after evaluating each "
"configuration on the same subset size, only a fraction of 1/eta of them "
"'advances' to the next round. Must be greater or equal to 2."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:85 ../../Tuner/BohbAdvisor.rst:123
msgid ""
"**min_points_in_model** (*int, optional, default = None*): number of "
"observations to start building a KDE. Default 'None' means dim+1; when "
"the number of completed trials in this budget is equal to or larger than "
"``max{dim+1, min_points_in_model}``, BOHB will start to build a KDE model"
" of this budget then use said KDE model to guide configuration selection."
" Needs to be positive. (dim means the number of hyperparameters in search"
" space)"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:86 ../../Tuner/BohbAdvisor.rst:124
#, python-format
msgid ""
"**top_n_percent** (*int, optional, default = 15*): percentage (between 1 "
"and 99) of the observations which are considered good. Good points and "
"bad points are used for building KDE models. For example, if you have 100"
" observed trials and top_n_percent is 15, then the top 15% of points will"
" be used for building the good points models \"l(x)\". The remaining 85% "
"of points will be used for building the bad point models \"g(x)\"."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:87 ../../Tuner/BohbAdvisor.rst:125
msgid ""
"**num_samples** (*int, optional, default = 64*): number of samples to "
"optimize EI (default 64). In this case, we will sample \"num_samples\" "
"points and compare the result of l(x)/g(x). Then we will return the one "
"with the maximum l(x)/g(x) value as the next configuration if the "
"optimize_mode is ``maximize``. Otherwise, we return the smallest one."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:88 ../../Tuner/BohbAdvisor.rst:126
msgid ""
"**random_fraction** (*float, optional, default = 0.33*): fraction of "
"purely random configurations that are sampled from the prior without the "
"model."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:89 ../../Tuner/BohbAdvisor.rst:127
msgid ""
"**bandwidth_factor** (*float, optional, default = 3.0*): to encourage "
"diversity, the points proposed to optimize EI are sampled from a "
"'widened' KDE where the bandwidth is multiplied by this factor. We "
"suggest using the default value if you are not familiar with KDE."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:90 ../../Tuner/BohbAdvisor.rst:128
msgid ""
"**min_bandwidth** (*float, optional, default = 0.001*): to keep "
"diversity, even when all (good) samples have the same value for one of "
"the parameters, a minimum bandwidth (default: 1e-3) is used instead of "
"zero. We suggest using the default value if you are not familiar with "
"KDE."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:92 ../../Tuner/BohbAdvisor.rst:131
msgid ""
"*Please note that the float type currently only supports decimal "
"representations. You have to use 0.333 instead of 1/3 and 0.001 instead "
"of 1e-3.*"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:96 ../../Tuner/NetworkmorphismTuner.rst:28
msgid "Config File"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:98
msgid ""
"To use BOHB, you should add the following spec in your experiment's YAML "
"config file:"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:116
msgid "**classArgs Requirements:**"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:129
msgid ""
"**config_space** (*str, optional*): directly use a .pcs file serialized "
"by `ConfigSpace <https://automl.github.io/ConfigSpace/>` in \"pcs new\" "
"format. In this case, search space file (if provided in config) will be "
"ignored. Note that this path needs to be an absolute path. Relative path "
"is currently not supported."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:134 ../../Tuner/NetworkmorphismTuner.rst:135
msgid "File Structure"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:136
msgid ""
"The advisor has a lot of different files, functions, and classes. Here, "
"we will only give most of those files a brief introduction:"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:139
msgid ""
"``bohb_advisor.py`` Definition of BOHB, handles interaction with the "
"dispatcher, including generating new trials and processing results. Also "
"includes the implementation of the HB (Hyperband) part."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:140
msgid ""
"``config_generator.py`` Includes the implementation of the BO (Bayesian "
"Optimization) part. The function *get_config* can generate new "
"configurations based on BO; the function *new_result* will update the "
"model with the new result."
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:143
msgid "Experiment"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:146
msgid "MNIST with BOHB"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:148
msgid ""
"code implementation: :githublink:`examples/trials/mnist-advisor "
"<examples/trials/>`"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:150
msgid ""
"We chose BOHB to build a CNN on the MNIST dataset. The following is our "
"experimental final results:"
msgstr ""

#: ../../Tuner/BohbAdvisor.rst:158
msgid ""
"More experimental results can be found in the `reference paper "
"<https://arxiv.org/abs/1807.01774>`__. We can see that BOHB makes good "
"use of previous results and has a balanced trade-off in exploration and "
"exploitation."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:2
msgid "HyperParameter Tuning with NNI Built-in Tuners"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:4
msgid ""
"To fit a machine/deep learning model into different tasks/problems, "
"hyperparameters always need to be tuned. Automating the process of "
"hyperparaeter tuning always requires a good tuning algorithm. NNI has "
"provided state-of-the-art tuning algorithms as part of our built-in "
"tuners and makes them easy to use. Below is the brief summary of NNI's "
"current built-in tuners:"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:6
msgid ""
"Note: Click the **Tuner's name** to get the Tuner's installation "
"requirements, suggested scenario, and an example configuration. A link "
"for a detailed description of each algorithm is located at the end of the"
" suggested scenario for each tuner. Here is an `article "
"<../CommunitySharings/HpoComparison.rst>`__ comparing different Tuners on"
" several problems."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:8
msgid "Currently, we support the following algorithms:"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:14
msgid "Tuner"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:15
msgid "Brief Introduction of Algorithm"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:17
msgid "`TPE <./TpeTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:18
msgid ""
"The Tree-structured Parzen Estimator (TPE) is a sequential model-based "
"optimization (SMBO) approach. SMBO methods sequentially construct models "
"to approximate the performance of hyperparameters based on historical "
"measurements, and then subsequently choose new hyperparameters to test "
"based on this model. `Reference Paper <https://papers.nips.cc/paper/4443"
"-algorithms-for-hyper-parameter-optimization.pdf>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:20
msgid ""
"TPE, as a black-box optimization, can be used in various scenarios and "
"shows good performance in general. Especially when you have limited "
"computation resources and can only try a small number of trials. From a "
"large amount of experiments, we found that TPE is far better than Random "
"Search."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:22
msgid "`Random Search <./RandomTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:23
msgid ""
"In Random Search for Hyper-Parameter Optimization show that Random Search"
" might be surprisingly simple and effective. We suggest that we could use"
" Random Search as the baseline when we have no knowledge about the prior "
"distribution of hyper-parameters. `Reference Paper "
"<http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:25
msgid ""
"Random search is suggested when each trial does not take very long (e.g.,"
" each trial can be completed very quickly, or early stopped by the "
"assessor), and you have enough computational resources. It's also useful "
"if you want to uniformly explore the search space. Random Search can be "
"considered a baseline search algorithm."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:27
msgid "`Anneal <./AnnealTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:28
msgid ""
"This simple annealing algorithm begins by sampling from the prior, but "
"tends over time to sample from points closer and closer to the best ones "
"observed. This algorithm is a simple variation on the random search that "
"leverages smoothness in the response surface. The annealing rate is not "
"adaptive."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:30
msgid ""
"Anneal is suggested when each trial does not take very long and you have "
"enough computation resources (very similar to Random Search). It's also "
"useful when the variables in the search space can be sample from some "
"prior distribution."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:32
msgid "`Naïve Evolution <./EvolutionTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:33
msgid ""
"Naïve Evolution comes from Large-Scale Evolution of Image Classifiers. It"
" randomly initializes a population-based on search space. For each "
"generation, it chooses better ones and does some mutation (e.g., change a"
" hyperparameter, add/remove one layer) on them to get the next "
"generation. Naïve Evolution requires many trials to work, but it's very "
"simple and easy to expand new features. `Reference paper "
"<https://arxiv.org/pdf/1703.01041.pdf>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:35
msgid ""
"Its computational resource requirements are relatively high. "
"Specifically, it requires a large initial population to avoid falling "
"into a local optimum. If your trial is short or leverages assessor, this "
"tuner is a good choice. It is also suggested when your trial code "
"supports weight transfer; that is, the trial could inherit the converged "
"weights from its parent(s). This can greatly speed up the training "
"process."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:37
msgid "`SMAC <./SmacTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:38
msgid ""
"SMAC is based on Sequential Model-Based Optimization (SMBO). It adapts "
"the most prominent previously used model class (Gaussian stochastic "
"process models) and introduces the model class of random forests to SMBO,"
" in order to handle categorical parameters. The SMAC supported by NNI is "
"a wrapper on the SMAC3 GitHub repo. Notice, SMAC needs to be installed by"
" ``pip install nni[SMAC]`` command. `Reference Paper, "
"<https://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf>`__ `GitHub Repo "
"<https://github.com/automl/SMAC3>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:40
msgid ""
"**Please note that SMAC doesn't support running on Windows currently**. "
"For the specific reason, please refer to this `GitHub issue "
"<https://github.com/automl/SMAC3/issues/483>`__."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:42
msgid ""
"Similar to TPE, SMAC is also a black-box tuner that can be tried in "
"various scenarios and is suggested when computational resources are "
"limited. It is optimized for discrete hyperparameters, thus, it's "
"suggested when most of your hyperparameters are discrete."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:44
msgid "`Batch tuner <./BatchTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:45
msgid ""
"Batch tuner allows users to simply provide several configurations (i.e., "
"choices of hyper-parameters) for their trial code. After finishing all "
"the configurations, the experiment is done. Batch tuner only supports the"
" type choice in search space spec."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:47
msgid ""
"If the configurations you want to try have been decided beforehand, you "
"can list them in search space file (using ``choice``) and run them using "
"batch tuner. `Detailed Description <./BatchTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:50
msgid "`Grid Search <./GridsearchTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:51
msgid "Grid Search performs an exhaustive searching through the search space."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:53
msgid ""
"This is suggested when the search space is small. It's suggested when it "
"is feasible to exhaustively sweep the whole search space."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:55
msgid "`Hyperband <./HyperbandAdvisor.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:56
msgid ""
"Hyperband tries to use limited resources to explore as many "
"configurations as possible and returns the most promising ones as a final"
" result. The basic idea is to generate many configurations and run them "
"for a small number of trials. The half least-promising configurations are"
" thrown out, the remaining are further trained along with a selection of "
"new configurations. The size of these populations is sensitive to "
"resource constraints (e.g. allotted search time). `Reference Paper "
"<https://arxiv.org/pdf/1603.06560.pdf>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:58
msgid ""
"This is suggested when you have limited computational resources but have "
"a relatively large search space. It performs well in scenarios where "
"intermediate results can indicate good or bad final results to some "
"extent. For example, when models that are more accurate early on in "
"training are also more accurate later on."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:60
msgid "`Network Morphism <./NetworkmorphismTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:61
msgid ""
"Network Morphism provides functions to automatically search for deep "
"learning architectures. It generates child networks that inherit the "
"knowledge from their parent network which it is a morph from. This "
"includes changes in depth, width, and skip-connections. Next, it "
"estimates the value of a child network using historic architecture and "
"metric pairs. Then it selects the most promising one to train. `Reference"
" Paper <https://arxiv.org/abs/1806.10282>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:63
msgid ""
"This is suggested when you want to apply deep learning methods to your "
"task but you have no idea how to choose or design a network. You may "
"modify this :githublink:`example "
"<examples/trials/network_morphism/cifar10/cifar10_keras.py>` to fit your "
"own dataset and your own data augmentation method. Also you can change "
"the batch size, learning rate, or optimizer. Currently, this tuner only "
"supports the computer vision domain."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:65
msgid "`Metis Tuner <./MetisTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:66
msgid ""
"Metis offers the following benefits when it comes to tuning parameters: "
"While most tools only predict the optimal configuration, Metis gives you "
"two outputs: (a) current prediction of optimal configuration, and (b) "
"suggestion for the next trial. No more guesswork. While most tools assume"
" training datasets do not have noisy data, Metis actually tells you if "
"you need to re-sample a particular hyper-parameter. `Reference Paper "
"<https://www.microsoft.com/en-us/research/publication/metis-robustly-"
"tuning-tail-latencies-cloud-systems/>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:68
msgid ""
"Similar to TPE and SMAC, Metis is a black-box tuner. If your system takes"
" a long time to finish each trial, Metis is more favorable than other "
"approaches such as random search. Furthermore, Metis provides guidance on"
" subsequent trials. Here is an :githublink:`example <examples/trials"
"/auto-gbdt/search_space_metis.json>` on the use of Metis. Users only need"
" to send the final result, such as ``accuracy``, to the tuner by calling "
"the NNI SDK."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:70
msgid ""
"Note that the only acceptable types of search space types are "
"``quniform``, ``uniform``, ``randint``, and numerical ``choice``. Only "
"numerical values are supported since the values will be used to evaluate "
"the 'distance' between different points."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:72
msgid "`BOHB <./BohbAdvisor.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:73
msgid ""
"BOHB is a follow-up work to Hyperband. It targets the weakness of "
"Hyperband that new configurations are generated randomly without "
"leveraging finished trials. For the name BOHB, HB means Hyperband, BO "
"means Bayesian Optimization. BOHB leverages finished trials by building "
"multiple TPE models, a proportion of new configurations are generated "
"through these models. `Reference Paper "
"<https://arxiv.org/abs/1807.01774>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:75
msgid ""
"Similar to Hyperband, BOHB is suggested when you have limited "
"computational resources but have a relatively large search space. It "
"performs well in scenarios where intermediate results can indicate good "
"or bad final results to some extent. In this case, it may converge to a "
"better configuration than Hyperband due to its usage of Bayesian "
"optimization."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:77
msgid "`GP Tuner <./GPTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:78
msgid ""
"Gaussian Process Tuner is a sequential model-based optimization (SMBO) "
"approach with Gaussian Process as the surrogate. `Reference Paper "
"<https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-"
"optimization.pdf>`__, `Github Repo "
"<https://github.com/fmfn/BayesianOptimization>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:80
msgid ""
"Note that the only acceptable types within the search space are "
"``randint``, ``uniform``, ``quniform``, ``loguniform``, ``qloguniform``, "
"and numerical ``choice``. Only numerical values are supported since the "
"values will be used to evaluate the 'distance' between different points."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:82
msgid ""
"As a strategy in a Sequential Model-based Global Optimization (SMBO) "
"algorithm, GP Tuner uses a proxy optimization problem (finding the "
"maximum of the acquisition function) that, albeit still a hard problem, "
"is cheaper (in the computational sense) to solve and common tools can be "
"employed to solve it. Therefore, GP Tuner is most adequate for situations"
" where the function to be optimized is very expensive to evaluate. GP can"
" be used when computational resources are limited. However, GP Tuner has "
"a computational cost that grows at *O(N^3)* due to the requirement of "
"inverting the Gram matrix, so it's not suitable when lots of trials are "
"needed."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:84
msgid "`PBT Tuner <./PBTTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:85
msgid ""
"PBT Tuner is a simple asynchronous optimization algorithm which "
"effectively utilizes a fixed computational budget to jointly optimize a "
"population of models and their hyperparameters to maximize performance. "
"`Reference Paper <https://arxiv.org/abs/1711.09846v1>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:87
msgid ""
"Population Based Training (PBT) bridges and extends parallel search "
"methods and sequential optimization methods. It requires relatively small"
" computation resource, by inheriting weights from currently good-"
"performing ones to explore better ones periodically. With PBTTuner, users"
" finally get a trained model, rather than a configuration that could "
"reproduce the trained model by training the model from scratch. This is "
"because model weights are inherited periodically through the whole search"
" process. PBT can also be seen as a training approach. If you don't need "
"to get a specific configuration, but just expect a good model, PBTTuner "
"is a good choice."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:89
msgid "`DNGO Tuner <./DngoTuner.rst>`__"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:90
msgid ""
"Use of neural networks as an alternative to GPs to model distributions "
"over functions in bayesian optimization."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:92
msgid ""
"Applicable to large scale hyperparameter optimization. Bayesian "
"optimization that rapidly finds competitive models on benchmark object "
"recognition tasks using convolutional networks, and image caption "
"generation using neural language models."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:95
msgid "Usage of Built-in Tuners"
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:97
msgid ""
"Using a built-in tuner provided by the NNI SDK requires one to declare "
"the  **name** and **classArgs** in the ``config.yml`` file. Click tuners'"
" name in above table to see their specification."
msgstr ""

#: ../../Tuner/BuiltinTuner.rst:100
msgid ""
"Note: Some built-in tuners have dependencies that need to be installed "
"using ``pip install nni[<tuner>]``, like SMAC's dependencies can be "
"installed using ``pip install nni[SMAC]``."
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:2
msgid "**How To** - Customize Your Own Advisor"
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:4
msgid "*Warning: API is subject to change in future releases.*"
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:6
msgid ""
"Advisor targets the scenario that the automl algorithm wants the methods "
"of both tuner and assessor. Advisor is similar to tuner on that it "
"receives trial parameters request, final results, and generate trial "
"parameters. Also, it is similar to assessor on that it receives "
"intermediate results, trial's end state, and could send trial kill "
"command. Note that, if you use Advisor, tuner and assessor are not "
"allowed to be used at the same time."
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:8
msgid "If a user want to implement a customized Advisor, she/he only needs to:"
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:10
msgid ""
"**1. Define an Advisor inheriting from the MsgDispatcherBase class.** For"
" example:"
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:20
msgid ""
"**2. Implement the methods with prefix \"handle_\" except "
"\"handle_request\"\"**"
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:22
msgid ""
"You might find `docs <../autotune_ref.rst#Advisor>`__ for "
"``MsgDispatcherBase`` helpful."
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:24
msgid "**3. Configure your customized Advisor in experiment YAML config file.**"
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:26
msgid ""
"Similar to tuner and assessor. NNI needs to locate your customized "
"Advisor class and instantiate the class, so you need to specify the "
"location of the customized Advisor class and pass literal values as "
"parameters to the ``__init__`` constructor."
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:39
msgid ""
"**Note that** The working directory of your advisor is ``<home>/nni-"
"experiments/<experiment_id>/log``, which can be retrieved with "
"environment variable ``NNI_LOG_DIRECTORY``."
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:42
msgid "Example"
msgstr ""

#: ../../Tuner/CustomizeAdvisor.rst:44
msgid ""
"Here we provide an :githublink:`example "
"<examples/tuners/mnist_keras_customized_advisor>`."
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:2
msgid "Customize-Tuner"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:4
msgid ""
"NNI provides state-of-the-art tuning algorithm in builtin-tuners. NNI "
"supports to build a tuner by yourself for tuning demand."
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:6
msgid ""
"If you want to implement your own tuning algorithm, you can implement a "
"customized Tuner, there are three things to do:"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:9
msgid "Inherit the base Tuner class"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:10
msgid ""
"Implement receive_trial_result, generate_parameter and "
"update_search_space function"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:11
msgid "Configure your customized tuner in experiment YAML config file"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:13
msgid "Here is an example:"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:15
msgid "**1. Inherit the base Tuner class**"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:25
msgid ""
"**2. Implement receive_trial_result, generate_parameter and "
"update_search_space function**"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:64
msgid ""
"``receive_trial_result`` will receive the ``parameter_id, parameters, "
"value`` as parameters input. Also, Tuner will receive the ``value`` "
"object are exactly same value that Trial send."
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:66
msgid ""
"The ``your_parameters`` return from ``generate_parameters`` function, "
"will be package as json object by NNI SDK. NNI SDK will unpack json "
"object so the Trial will receive the exact same ``your_parameters`` from "
"Tuner."
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:68
msgid "For example: If the you implement the ``generate_parameters`` like this:"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:81
msgid ""
"It means your Tuner will always generate parameters ``{\"dropout\": 0.3, "
"\"learning_rate\": 0.4}``. Then Trial will receive ``{\"dropout\": 0.3, "
"\"learning_rate\": 0.4}`` by calling API ``nni.get_next_parameter()``. "
"Once the trial ends with a result (normally some kind of metrics), it can"
" send the result to Tuner by calling API ``nni.report_final_result()``, "
"for example ``nni.report_final_result(0.93)``. Then your Tuner's "
"``receive_trial_result`` function will receied the result like："
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:89
msgid ""
"**Note that** The working directory of your tuner is ``<home>/nni-"
"experiments/<experiment_id>/log``, which can be retrieved with "
"environment variable ``NNI_LOG_DIRECTORY``, therefore, if you want to "
"access a file (e.g., ``data.txt``) in the directory of your own tuner, "
"you cannot use ``open('data.txt', 'r')``. Instead, you should use the "
"following:"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:96
msgid ""
"This is because your tuner is not executed in the directory of your tuner"
" (i.e., ``pwd`` is not the directory of your own tuner)."
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:98
msgid "**3. Configure your customized tuner in experiment YAML config file**"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:100
msgid ""
"NNI needs to locate your customized tuner class and instantiate the "
"class, so you need to specify the location of the customized tuner class "
"and pass literal values as parameters to the __init__ constructor."
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:113
msgid "More detail example you could see:"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:117
msgid ":githublink:`evolution-tuner <nni/algorithms/hpo/evolution_tuner.py>`"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:118
msgid ":githublink:`hyperopt-tuner <nni/algorithms/hpo/hyperopt_tuner.py>`"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:119
msgid ""
":githublink:`evolution-based-customized-tuner "
"<examples/tuners/ga_customer_tuner>`"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:123
msgid "Write a more advanced automl algorithm"
msgstr ""

#: ../../Tuner/CustomizeTuner.rst:125
msgid ""
"The methods above are usually enough to write a general tuner. However, "
"users may also want more methods, for example, intermediate results, "
"trials' state (e.g., the methods in assessor), in order to have a more "
"powerful automl algorithm. Therefore, we have another concept called "
"``advisor`` which directly inherits from ``MsgDispatcherBase`` in "
":githublink:`msg_dispatcher_base.py "
"<nni/runtime/msg_dispatcher_base.py>`. Please refer to `here "
"<CustomizeAdvisor.rst>`__ for how to write a customized advisor."
msgstr ""

#: ../../Tuner/DngoTuner.rst:2
msgid "DNGO Tuner"
msgstr ""

#: ../../Tuner/DngoTuner.rst:11 ../../Tuner/GPTuner.rst:16
#: ../../Tuner/HyperbandAdvisor.rst:117 ../../Tuner/MetisTuner.rst:27
#: ../../Tuner/PBTTuner.rst:44 ../../Tuner/SmacTuner.rst:21
msgid "classArgs requirements"
msgstr ""

#: ../../Tuner/DngoTuner.rst:13 ../../Tuner/PBTTuner.rst:46
msgid ""
"**optimize_mode** (*'maximize' or 'minimize'*) - If 'maximize', the tuner"
" will target to maximize metrics. If 'minimize', the tuner will target to"
" minimize metrics."
msgstr ""

#: ../../Tuner/DngoTuner.rst:14
msgid ""
"**sample_size** (*int, default = 1000*) - Number of samples to select in "
"each iteration. The best one will be picked from the samples as the next "
"trial."
msgstr ""

#: ../../Tuner/DngoTuner.rst:15
msgid ""
"**trials_per_update** (*int, default = 20*) - Number of trials to collect"
" before updating the model."
msgstr ""

#: ../../Tuner/DngoTuner.rst:16
msgid ""
"**num_epochs_per_training** (*int, default = 500*) - Number of epochs to "
"train DNGO model."
msgstr ""

#: ../../Tuner/EvolutionTuner.rst:2
msgid "Naive Evolution Tuner"
msgstr ""

#: ../../Tuner/EvolutionTuner.rst:4
msgid ""
"Naive Evolution comes from `Large-Scale Evolution of Image Classifiers "
"<https://arxiv.org/pdf/1703.01041.pdf>`__. It randomly initializes a "
"population based on the search space. For each generation, it chooses "
"better ones and does some mutation (e.g., changes a hyperparameter, "
"adds/removes one layer, etc.) on them to get the next generation. Naive "
"Evolution requires many trials to works but it's very simple and it's "
"easily expanded with new features."
msgstr ""

#: ../../Tuner/EvolutionTuner.rst:16
msgid ""
"**population_size** (*int value (should > 0), optional, default = 20*) - "
"the initial size of the population (trial num) in the evolution tuner. "
"It's suggested that ``population_size`` be much larger than "
"``concurrency`` so users can get the most out of the algorithm (and at "
"least ``concurrency``, or the tuner will fail on its first generation of "
"parameters)."
msgstr ""

#: ../../Tuner/GPTuner.rst:2
msgid "GP Tuner"
msgstr ""

#: ../../Tuner/GPTuner.rst:4
msgid ""
"Bayesian optimization works by constructing a posterior distribution of "
"functions (a Gaussian Process) that best describes the function you want "
"to optimize. As the number of observations grows, the posterior "
"distribution improves, and the algorithm becomes more certain of which "
"regions in parameter space are worth exploring and which are not."
msgstr ""

#: ../../Tuner/GPTuner.rst:6
msgid ""
"GP Tuner is designed to minimize/maximize the number of steps required to"
" find a combination of parameters that are close to the optimal "
"combination. To do so, this method uses a proxy optimization problem "
"(finding the maximum of the acquisition function) that, albeit still a "
"hard problem, is cheaper (in the computational sense) to solve, and it's "
"amenable to common tools. Therefore, Bayesian Optimization is suggested "
"for situations where sampling the function to be optimized is very "
"expensive."
msgstr ""

#: ../../Tuner/GPTuner.rst:8
msgid ""
"Note that the only acceptable types within the search space are "
"``randint``, ``uniform``, ``quniform``, ``loguniform``, ``qloguniform``, "
"and numerical ``choice``."
msgstr ""

#: ../../Tuner/GPTuner.rst:10
msgid ""
"This optimization approach is described in Section 3 of `Algorithms for "
"Hyper-Parameter Optimization <https://papers.nips.cc/paper/4443"
"-algorithms-for-hyper-parameter-optimization.pdf>`__."
msgstr ""

#: ../../Tuner/GPTuner.rst:18 ../../Tuner/MetisTuner.rst:29
msgid ""
"**optimize_mode** (*'maximize' or 'minimize', optional, default = "
"'maximize'*) - If 'maximize', the tuner will try to maximize metrics. If "
"'minimize', the tuner will try to minimize metrics."
msgstr ""

#: ../../Tuner/GPTuner.rst:19
msgid ""
"**utility** (*'ei', 'ucb' or 'poi', optional, default = 'ei'*) - The "
"utility function (acquisition function). 'ei', 'ucb', and 'poi' "
"correspond to 'Expected Improvement', 'Upper Confidence Bound', and "
"'Probability of Improvement', respectively."
msgstr ""

#: ../../Tuner/GPTuner.rst:20
msgid ""
"**kappa** (*float, optional, default = 5*) - Used by the 'ucb' utility "
"function. The bigger ``kappa`` is, the more exploratory the tuner will "
"be."
msgstr ""

#: ../../Tuner/GPTuner.rst:21
msgid ""
"**xi** (*float, optional, default = 0*) - Used by the 'ei' and 'poi' "
"utility functions. The bigger ``xi`` is, the more exploratory the tuner "
"will be."
msgstr ""

#: ../../Tuner/GPTuner.rst:22
msgid ""
"**nu** (*float, optional, default = 2.5*) - Used to specify the Matern "
"kernel. The smaller nu, the less smooth the approximated function is."
msgstr ""

#: ../../Tuner/GPTuner.rst:23
msgid ""
"**alpha** (*float, optional, default = 1e-6*) - Used to specify the "
"Gaussian Process Regressor. Larger values correspond to an increased "
"noise level in the observations."
msgstr ""

#: ../../Tuner/GPTuner.rst:24
msgid ""
"**cold_start_num** (*int, optional, default = 10*) - Number of random "
"explorations to perform before the Gaussian Process. Random exploration "
"can help by diversifying the exploration space."
msgstr ""

#: ../../Tuner/GPTuner.rst:25
msgid ""
"**selection_num_warm_up** (*int, optional, default = 1e5*) - Number of "
"random points to evaluate when getting the point which maximizes the "
"acquisition function."
msgstr ""

#: ../../Tuner/GPTuner.rst:26
msgid ""
"**selection_num_starting_points** (*int, optional, default = 250*) - "
"Number of times to run L-BFGS-B from a random starting point after the "
"warmup."
msgstr ""

#: ../../Tuner/GridsearchTuner.rst:2
msgid "Grid Search Tuner"
msgstr ""

#: ../../Tuner/GridsearchTuner.rst:4
msgid "Grid Search performs an exhaustive search through a search space."
msgstr ""

#: ../../Tuner/GridsearchTuner.rst:6
msgid ""
"For uniform and normal distributed parameters, grid search tuner samples "
"them at progressively decreased intervals."
msgstr ""

#: ../../Tuner/GridsearchTuner.rst:11
msgid "Grid search tuner has no argument."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:2
msgid "Hyperband Advisor"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:4
msgid ""
"`Hyperband <https://arxiv.org/pdf/1603.06560.pdf>`__ is a popular autoML "
"algorithm. The basic idea of Hyperband is to create several buckets, each"
" having ``n`` randomly generated hyperparameter configurations, each "
"configuration using ``r`` resources (e.g., epoch number, batch number). "
"After the ``n`` configurations are finished, it chooses the top ``n/eta``"
" configurations and runs them using increased ``r*eta`` resources. At "
"last, it chooses the best configuration it has found so far."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:7
msgid "Implementation with full parallelism"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:9
msgid ""
"First, this is an example of how to write an autoML algorithm based on "
"MsgDispatcherBase, rather than Tuner and Assessor. Hyperband is "
"implemented in this way because it integrates the functions of both Tuner"
" and Assessor, thus, we call it Advisor."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:11
msgid ""
"Second, this implementation fully leverages Hyperband's internal "
"parallelism. Specifically, the next bucket is not started strictly after "
"the current bucket. Instead, it starts when there are available "
"resources. If you want to use full parallelism mode, set ``exec_mode`` "
"with ``parallelism``."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:13
msgid ""
"Or if you want to set ``exec_mode`` with ``serial`` according to the "
"original algorithm. In this mode, the next bucket will start strictly "
"after the current bucket."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:15
msgid ""
"``parallelism`` mode may lead to multiple unfinished buckets, and there "
"is at most one unfinished bucket under ``serial`` mode. The advantage of "
"``parallelism`` mode is to make full use of resources, which may reduce "
"the experiment duration multiple times. The following two pictures are "
"the results of quick verification using `nas-bench-201 "
"<../NAS/Benchmarks.rst>`__, picture above is in ``parallelism`` mode, "
"picture below is in ``serial`` mode."
msgstr ""

msgid "parallelism mode"
msgstr ""

msgid "serial mode"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:29
msgid ""
"If you want to reproduce these results, refer to the example under "
"``examples/trials/benchmarking/`` for details."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:35
msgid "Config file"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:37
msgid ""
"To use Hyperband, you should add the following spec in your experiment's "
"YAML config file:"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:54
msgid ""
"Note that once you use Advisor, you are not allowed to add a Tuner and "
"Assessor spec in the config file. If you use Hyperband, among the "
"hyperparameters (i.e., key-value pairs) received by a trial, there will "
"be one more key called ``TRIAL_BUDGET`` defined by user. **By using this "
"``TRIAL_BUDGET``, the trial can control how long it runs**."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:56
msgid ""
"For ``report_intermediate_result(metric)`` and "
"``report_final_result(metric)`` in your trial code, **``metric`` should "
"be either a number or a dict which has a key ``default`` with a number as"
" its value**. This number is the one you want to maximize or minimize, "
"for example, accuracy or loss."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:58
msgid ""
"``R`` and ``eta`` are the parameters of Hyperband that you can change. "
"``R`` means the maximum trial budget that can be allocated to a "
"configuration. Here, trial budget could mean the number of epochs or "
"mini-batches. This ``TRIAL_BUDGET`` should be used by the trial to "
"control how long it runs. Refer to the example under ``examples/trials"
"/mnist-advisor/`` for details."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:60
msgid ""
"``eta`` means ``n/eta`` configurations from ``n`` configurations will "
"survive and rerun using more budgets."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:62
msgid "Here is a concrete example of ``R=81`` and ``eta=3``:"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:69
msgid "s=4"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:70
msgid "s=3"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:71
msgid "s=2"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:72
msgid "s=1"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:73
msgid "s=0"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:74
msgid "i"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:75 ../../Tuner/HyperbandAdvisor.rst:76
#: ../../Tuner/HyperbandAdvisor.rst:77 ../../Tuner/HyperbandAdvisor.rst:78
#: ../../Tuner/HyperbandAdvisor.rst:79
msgid "n r"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:80
msgid "0"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:81
msgid "81 1"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:82 ../../Tuner/HyperbandAdvisor.rst:87
msgid "27 3"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:83 ../../Tuner/HyperbandAdvisor.rst:88
#: ../../Tuner/HyperbandAdvisor.rst:93
msgid "9 9"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:84
msgid "6 27"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:85
msgid "5 81"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:86
msgid "1"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:89 ../../Tuner/HyperbandAdvisor.rst:94
#: ../../Tuner/HyperbandAdvisor.rst:99
msgid "3 27"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:90
msgid "2 81"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:92
msgid "2"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:95 ../../Tuner/HyperbandAdvisor.rst:100
#: ../../Tuner/HyperbandAdvisor.rst:105
msgid "1 81"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:98
msgid "3"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:104
msgid "4"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:112
msgid ""
"``s`` means bucket, ``n`` means the number of configurations that are "
"generated, the corresponding ``r`` means how many budgets these "
"configurations run. ``i`` means round, for example, bucket 4 has 5 "
"rounds, bucket 3 has 4 rounds."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:114
msgid ""
"For information about writing trial code, please refer to the "
"instructions under ``examples/trials/mnist-hyperband/``."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:121
msgid ""
"**R** (*int, optional, default = 60*) - the maximum budget given to a "
"trial (could be the number of mini-batches or epochs). Each trial should "
"use TRIAL_BUDGET to control how long they run."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:122
msgid ""
"**eta** (*int, optional, default = 3*) - ``(eta-1)/eta`` is the "
"proportion of discarded trials."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:123
msgid ""
"**exec_mode** (*serial or parallelism, optional, default = parallelism*) "
"- If 'parallelism', the tuner will try to use available resources to "
"start new bucket immediately. If 'serial', the tuner will only start new "
"bucket after the current bucket is done."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:139
msgid "Future improvements"
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:141
msgid ""
"The current implementation of Hyperband can be further improved by "
"supporting a simple early stop algorithm since it's possible that not all"
" the configurations in the top ``n/eta`` perform well. Any unpromising "
"configurations should be stopped early."
msgstr ""

#: ../../Tuner/HyperbandAdvisor.rst:143
msgid ""
"In the current implementation, configurations are generated randomly "
"which follows the design in the `paper "
"<https://arxiv.org/pdf/1603.06560.pdf>`__. As an improvement, "
"configurations could be generated more wisely by leveraging advanced "
"algorithms."
msgstr ""

#: ../../Tuner/MetisTuner.rst:2
msgid "Metis Tuner"
msgstr ""

#: ../../Tuner/MetisTuner.rst:4
msgid ""
"`Metis <https://www.microsoft.com/en-us/research/publication/metis-"
"robustly-tuning-tail-latencies-cloud-systems/>`__ offers several benefits"
" over other tuning algorithms. While most tools only predict the optimal "
"configuration, Metis gives you two outputs, a prediction for the optimal "
"configuration and a suggestion for the next trial. No more guess work!"
msgstr ""

#: ../../Tuner/MetisTuner.rst:6
msgid ""
"While most tools assume training datasets do not have noisy data, Metis "
"actually tells you if you need to resample a particular hyper-parameter."
msgstr ""

#: ../../Tuner/MetisTuner.rst:8
msgid ""
"While most tools have problems of being exploitation-heavy, Metis' search"
" strategy balances exploration, exploitation, and (optional) resampling."
msgstr ""

#: ../../Tuner/MetisTuner.rst:10
msgid ""
"Metis belongs to the class of sequential model-based optimization (SMBO) "
"algorithms and it is based on the Bayesian Optimization framework. To "
"model the parameter-vs-performance space, Metis uses both a Gaussian "
"Process and GMM. Since each trial can impose a high time cost, Metis "
"heavily trades inference computations with naive trials. At each "
"iteration, Metis does two tasks:"
msgstr ""

#: ../../Tuner/MetisTuner.rst:14
msgid ""
"It finds the global optimal point in the Gaussian Process space. This "
"point represents the optimal configuration."
msgstr ""

#: ../../Tuner/MetisTuner.rst:17
msgid ""
"It identifies the next hyper-parameter candidate. This is achieved by "
"inferring the potential information gain of exploration, exploitation, "
"and resampling."
msgstr ""

#: ../../Tuner/MetisTuner.rst:19
msgid ""
"Note that the only acceptable types within the search space are "
"``quniform``, ``uniform``, ``randint``, and numerical ``choice``."
msgstr ""

#: ../../Tuner/MetisTuner.rst:21
msgid ""
"More details can be found in our `paper <https://www.microsoft.com/en-"
"us/research/publication/metis-robustly-tuning-tail-latencies-cloud-"
"systems/>`__."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:2
msgid "Network Morphism Tuner"
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:4
msgid ""
"`Autokeras <https://arxiv.org/abs/1806.10282>`__ is a popular autoML tool"
" using Network Morphism. The basic idea of Autokeras is to use Bayesian "
"Regression to estimate the metric of the Neural Network Architecture. "
"Each time, it generates several child networks from father networks. Then"
" it uses a naïve Bayesian regression to estimate its metric value from "
"the history of trained results of network and metric value pairs. Next, "
"it chooses the child which has the best, estimated performance and adds "
"it to the training queue. Inspired by the work of Autokeras and referring"
" to its `code <https://github.com/jhfjhfj1/autokeras>`__, we implemented "
"our Network Morphism method on the NNI platform."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:6
msgid ""
"If you want to know more about network morphism trial usage, please see "
"the :githublink:`Readme.md "
"<examples/trials/network_morphism/README.rst>`."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:14
msgid ""
"NetworkMorphism requires :githublink:`PyTorch "
"<examples/trials/network_morphism/requirements.txt>`."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:20
msgid ""
"**task** (*('cv'), optional, default = 'cv'*) - The domain of the "
"experiment. For now, this tuner only supports the computer vision (CV) "
"domain."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:21
msgid "**input_width** (*int, optional, default = 32*) - input image width"
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:22
msgid "**input_channel** (*int, optional, default = 3*) - input image channel"
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:23
msgid "**n_output_node** (*int, optional, default = 10*) - number of classes"
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:30
msgid ""
"To use Network Morphism, you should modify the following spec in your "
"``config.yml`` file:"
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:64
msgid ""
"In the training procedure, it generates a JSON file which represents a "
"Network Graph. Users can call the \"json_to_graph()\" function to build a"
" PyTorch or Keras model from this JSON file."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:89
msgid ""
"If you want to save and load the **best model**, the following methods "
"are recommended."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:137
msgid ""
"The tuner has a lot of different files, functions, and classes. Here, we "
"will give most of those files only a brief introduction:"
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:141
msgid ""
"``networkmorphism_tuner.py`` is a tuner which uses network morphism "
"techniques."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:144
msgid ""
"``bayesian.py`` is a Bayesian method to estimate the metric of unseen "
"model based on the models we have already searched."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:146
msgid ""
"``graph.py``  is the meta graph data structure. The class Graph "
"represents the neural architecture graph of a model."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:148
msgid "Graph extracts the neural architecture graph from a model."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:149
msgid "Each node in the graph is an intermediate tensor between layers."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:150
msgid "Each layer is an edge in the graph."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:151
msgid "Notably, multiple edges may refer to the same layer."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:154
msgid ""
"``graph_transformer.py`` includes some graph transformers which widen, "
"deepen, or add skip-connections to the graph."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:157
msgid "``layers.py``  includes all the layers we use in our model."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:159
msgid ""
"``layer_transformer.py`` includes some layer transformers which widen, "
"deepen, or add skip-connections to the layer."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:160
msgid "``nn.py`` includes the class which generates the initial network."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:161
msgid "``metric.py`` some metric classes including Accuracy and MSE."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:162
msgid ""
"``utils.py`` is the example search network architectures for the "
"``cifar10`` dataset, using Keras."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:165
msgid "The Network Representation Json Example"
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:167
msgid ""
"Here is an example of the intermediate representation JSON file we "
"defined, which is passed from the tuner to the trial in the architecture "
"search procedure. Users can call the \"json_to_graph()\" function in the "
"trial code to build a PyTorch or Keras model from this JSON file."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:260
msgid ""
"You can consider the model to be a `directed acyclic graph "
"<https://en.wikipedia.org/wiki/Directed_acyclic_graph>`__. The definition"
" of each model is a JSON object where:"
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:263
msgid "``input_shape`` is a list of integers which do not include the batch axis."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:264
msgid ""
"``weighted`` means whether the weights and biases in the neural network "
"should be included in the graph."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:265
msgid ""
"``operation_history`` is a list saving all the network morphism "
"operations."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:266
msgid ""
"``layer_id_to_input_node_ids`` is a dictionary mapping from layer "
"identifiers to their input nodes identifiers."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:267
msgid ""
"``layer_id_to_output_node_ids`` is a dictionary mapping from layer "
"identifiers to their output nodes identifiers"
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:268
msgid ""
"``adj_list`` is a two-dimensional list; the adjacency list of the graph. "
"The first dimension is identified by tensor identifiers. In each edge "
"list, the elements are two-element tuples of (tensor identifier, layer "
"identifier)."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:269
msgid ""
"``reverse_adj_list`` is a reverse adjacent list in the same format as "
"adj_list."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:270
msgid ""
"``node_list`` is a list of integers. The indices of the list are the "
"identifiers."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:272
msgid ""
"``layer_list`` is a list of stub layers. The indices of the list are the "
"identifiers."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:276
msgid ""
"For ``StubConv (StubConv1d, StubConv2d, StubConv3d)``, the numbering "
"follows the format: its node input id (or id list), node output id, "
"input_channel, filters, kernel_size, stride, and padding."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:279
msgid ""
"For ``StubDense``, the numbering follows the format: its node input id "
"(or id list), node output id, input_units, and units."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:282
msgid ""
"For ``StubBatchNormalization (StubBatchNormalization1d, "
"StubBatchNormalization2d, StubBatchNormalization3d)``, the numbering "
"follows the format: its node input id (or id list), node output id, and "
"features numbers."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:285
msgid ""
"For ``StubDropout(StubDropout1d, StubDropout2d, StubDropout3d)``, the "
"numbering follows the format: its node input id (or id list), node output"
" id, and dropout rate."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:288
msgid ""
"For ``StubPooling (StubPooling1d, StubPooling2d, StubPooling3d)``, the "
"numbering follows the format: its node input id (or id list), node output"
" id, kernel_size, stride, and padding."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:291
msgid ""
"For else layers, the numbering follows the format: its node input id (or "
"id list) and node output id."
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:294
msgid "TODO"
msgstr ""

#: ../../Tuner/NetworkmorphismTuner.rst:296
msgid ""
"Next step, we will change the API from s fixed network generator to a "
"network generator with more available operators. We will use ONNX instead"
" of JSON later as the intermediate representation spec in the future."
msgstr ""

#: ../../Tuner/PBTTuner.rst:2
msgid "PBT Tuner"
msgstr ""

#: ../../Tuner/PBTTuner.rst:4
msgid ""
"Population Based Training (PBT) comes from `Population Based Training of "
"Neural Networks <https://arxiv.org/abs/1711.09846v1>`__. It's a simple "
"asynchronous optimization algorithm which effectively utilizes a fixed "
"computational budget to jointly optimize a population of models and their"
" hyperparameters to maximize performance. Importantly, PBT discovers a "
"schedule of hyperparameter settings rather than following the generally "
"sub-optimal strategy of trying to find a single fixed set to use for the "
"whole course of training."
msgstr ""

#: ../../Tuner/PBTTuner.rst:12
msgid ""
"PBTTuner initializes a population with several trials (i.e., "
"``population_size``). There are four steps in the above figure, each "
"trial only runs by one step. How long is one step is controlled by trial "
"code, e.g., one epoch. When a trial starts, it loads a checkpoint "
"specified by PBTTuner and continues to run one step, then saves "
"checkpoint to a directory specified by PBTTuner and exits. The trials in "
"a population run steps synchronously, that is, after all the trials "
"finish the ``i``-th step, the ``(i+1)``-th step can be started. "
"Exploitation and exploration of PBT are executed between two consecutive "
"steps."
msgstr ""

#: ../../Tuner/PBTTuner.rst:18
msgid "Provide checkpoint directory"
msgstr ""

#: ../../Tuner/PBTTuner.rst:20
msgid ""
"Since some trials need to load other trial's checkpoint, users should "
"provide a directory (i.e., ``all_checkpoint_dir``) which is accessible by"
" every trial. It is easy for local mode, users could directly use the "
"default directory or specify any directory on the local machine. For "
"other training services, users should follow `the document of those "
"training services <../TrainingService/Overview.rst>`__ to provide a "
"directory in a shared storage, such as NFS, Azure storage."
msgstr ""

#: ../../Tuner/PBTTuner.rst:23
msgid "Modify your trial code"
msgstr ""

#: ../../Tuner/PBTTuner.rst:25
msgid ""
"Before running a step, a trial needs to load a checkpoint, the checkpoint"
" directory is specified in hyper-parameter configuration generated by "
"PBTTuner, i.e., ``params['load_checkpoint_dir']``. Similarly, the "
"directory for saving checkpoint is also included in the configuration, "
"i.e., ``params['save_checkpoint_dir']``. Here, ``all_checkpoint_dir`` is "
"base folder of ``load_checkpoint_dir`` and ``save_checkpoint_dir`` whose "
"format is ``all_checkpoint_dir/<population-id>/<step>``."
msgstr ""

#: ../../Tuner/PBTTuner.rst:41
msgid ""
"The complete example code can be found :githublink:`here <examples/trials"
"/mnist-pbt-tuner-pytorch>`."
msgstr ""

#: ../../Tuner/PBTTuner.rst:47
msgid ""
"**all_checkpoint_dir** (*str, optional, default = None*) - Directory for "
"trials to load and save checkpoint, if not specified, the directory would"
" be \"~/nni/checkpoint/\\ :raw-html:`<exp-id>`\\ \". Note that if the "
"experiment is not local mode, users should provide a path in a shared "
"storage which can be accessed by all the trials."
msgstr ""

#: ../../Tuner/PBTTuner.rst:48
msgid ""
"**population_size** (*int, optional, default = 10*) - Number of trials in"
" a population. Each step has this number of trials. In our "
"implementation, one step is running each trial by specific training "
"epochs set by users."
msgstr ""

#: ../../Tuner/PBTTuner.rst:49
msgid ""
"**factors** (*tuple, optional, default = (1.2, 0.8)*) - Factors for "
"perturbation of hyperparameters."
msgstr ""

#: ../../Tuner/PBTTuner.rst:50
msgid ""
"**fraction** (*float, optional, default = 0.2*) - Fraction for selecting "
"bottom and top trials."
msgstr ""

#: ../../Tuner/PBTTuner.rst:53
msgid "Experiment config"
msgstr ""

#: ../../Tuner/PBTTuner.rst:55
msgid ""
"Below is an exmaple of PBTTuner configuration in experiment config file. "
"**Note that Assessor is not allowed if PBTTuner is used.**"
msgstr ""

#: ../../Tuner/RandomTuner.rst:2
msgid "Random Tuner"
msgstr ""

#: ../../Tuner/RandomTuner.rst:4
msgid ""
"In `Random Search for Hyper-Parameter Optimization "
"<http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf>`__ we "
"show that Random Search might be surprisingly effective despite its "
"simplicity. We suggest using Random Search as a baseline when no "
"knowledge about the prior distribution of hyper-parameters is available."
msgstr ""

#: ../../Tuner/SmacTuner.rst:2
msgid "SMAC Tuner"
msgstr ""

#: ../../Tuner/SmacTuner.rst:4
msgid ""
"`SMAC <https://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf>`__ is based "
"on Sequential Model-Based Optimization (SMBO). It adapts the most "
"prominent previously used model class (Gaussian stochastic process "
"models) and introduces the model class of random forests to SMBO in order"
" to handle categorical parameters. The SMAC supported by nni is a wrapper"
" on `the SMAC3 github repo <https://github.com/automl/SMAC3>`__."
msgstr ""

#: ../../Tuner/SmacTuner.rst:6
msgid ""
"Note that SMAC on nni only supports a subset of the types in the `search "
"space spec <../Tutorial/SearchSpaceSpec.rst>`__: ``choice``, ``randint``,"
" ``uniform``, ``loguniform``, and ``quniform``."
msgstr ""

#: ../../Tuner/SmacTuner.rst:14
msgid ""
"SMAC has dependencies that need to be installed by following command "
"before the first usage. As a reminder, ``swig`` is required for SMAC: for"
" Ubuntu ``swig`` can be installed with ``apt``."
msgstr ""

#: ../../Tuner/SmacTuner.rst:24
msgid ""
"**config_dedup** (*True or False, optional, default = False*) - If True, "
"the tuner will not generate a configuration that has been already "
"generated. If False, a configuration may be generated twice, but it is "
"rare for a relatively large search space."
msgstr ""

#: ../../Tuner/TpeTuner.rst:2
msgid "TPE Tuner"
msgstr ""

#: ../../Tuner/TpeTuner.rst:4
msgid ""
"The Tree-structured Parzen Estimator (TPE) is a sequential model-based "
"optimization (SMBO) approach. SMBO methods sequentially construct models "
"to approximate the performance of hyperparameters based on historical "
"measurements, and then subsequently choose new hyperparameters to test "
"based on this model."
msgstr ""

#: ../../Tuner/TpeTuner.rst:8
msgid ""
"The TPE approach models P(x|y) and P(y) where x represents "
"hyperparameters and y the associated evaluation matric. P(x|y) is modeled"
" by transforming the generative process of hyperparameters, replacing the"
" distributions of the configuration prior with non-parametric densities."
msgstr ""

#: ../../Tuner/TpeTuner.rst:12
msgid ""
"This optimization approach is described in detail in `Algorithms for "
"Hyper-Parameter Optimization <https://papers.nips.cc/paper/4443"
"-algorithms-for-hyper-parameter-optimization.pdf>`__."
msgstr ""

#: ../../Tuner/TpeTuner.rst:15
msgid "Parallel TPE optimization"
msgstr ""

#: ../../Tuner/TpeTuner.rst:17
msgid ""
"TPE approaches were actually run asynchronously in order to make use of "
"multiple compute nodes and to avoid wasting time waiting for trial "
"evaluations to complete. The original algorithm design was optimized for "
"sequential computation. If we were to use TPE with much concurrency, its "
"performance will be bad. We have optimized this case using the Constant "
"Liar algorithm. For these principles of optimization, please refer to our"
" `research blog <../CommunitySharings/ParallelizingTpeSearch.rst>`__."
msgstr ""

#: ../../Tuner/TpeTuner.rst:26
msgid ""
"To use TPE, you should add the following spec in your experiment's YAML "
"config file:"
msgstr ""

#: ../../Tuner/TpeTuner.rst:53
msgid "classArgs"
msgstr ""

#: ../../Tuner/TpeTuner.rst:59
msgid "Field"
msgstr ""

#: ../../Tuner/TpeTuner.rst:60
msgid "Type"
msgstr ""

#: ../../Tuner/TpeTuner.rst:61
msgid "Default"
msgstr ""

#: ../../Tuner/TpeTuner.rst:62
msgid "Description"
msgstr ""

#: ../../Tuner/TpeTuner.rst:64
msgid "``optimize_mode``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:65
msgid "``'minimize' | 'maximize'``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:66
msgid "``'minimize'``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:67
msgid "Whether to minimize or maximize trial metrics."
msgstr ""

#: ../../Tuner/TpeTuner.rst:69
msgid "``seed``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:70
msgid "``int | null``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:71
msgid "``null``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:72
msgid "The random seed."
msgstr ""

#: ../../Tuner/TpeTuner.rst:74
msgid "``tpe_args.constant_liar_type``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:75
msgid "``'best' | 'worst' | 'mean' | null``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:76
msgid "``'best'``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:77
msgid ""
"TPE algorithm itself does not support parallel tuning. This parameter "
"specifies how to optimize for trial_concurrency > 1. How each liar works "
"is explained in paper's section 6.1."
msgstr ""

#: ../../Tuner/TpeTuner.rst:79
msgid ""
"In general ``best`` suit for small trial number and ``worst`` suit for "
"large trial number."
msgstr ""

#: ../../Tuner/TpeTuner.rst:81
msgid "``tpe_args.n_startup_jobs``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:82 ../../Tuner/TpeTuner.rst:89
#: ../../Tuner/TpeTuner.rst:94
msgid "``int``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:83
msgid "``20``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:84
msgid "The first N hyper-parameters are generated fully randomly for warming up."
msgstr ""

#: ../../Tuner/TpeTuner.rst:86
msgid ""
"If the search space is large, you can increase this value. Or if "
"max_trial_number is small, you may want to decrease it."
msgstr ""

#: ../../Tuner/TpeTuner.rst:88
msgid "``tpe_args.n_ei_candidates``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:90
msgid "``24``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:91
msgid ""
"For each iteration TPE samples EI for N sets of parameters and choose the"
" best one. (loosely speaking)"
msgstr ""

#: ../../Tuner/TpeTuner.rst:93
msgid "``tpe_args.linear_forgetting``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:95
msgid "``25``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:96
msgid ""
"TPE will lower the weights of old trials. This controls how many "
"iterations it takes for a trial to start decay."
msgstr ""

#: ../../Tuner/TpeTuner.rst:98
msgid "``tpe_args.prior_weight``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:99 ../../Tuner/TpeTuner.rst:110
msgid "``float``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:100
msgid "``1.0``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:101
msgid ""
"TPE treats user provided search space as prior. When generating new "
"trials, it also incorporates the prior in trial history by transforming "
"the search space to one trial configuration (i.e., each parameter of this"
" configuration chooses the mean of its candidate range). Here, "
"prior_weight determines the weight of this trial configuration in the "
"history trial configurations."
msgstr ""

#: ../../Tuner/TpeTuner.rst:106
msgid ""
"With prior weight 1.0, the search space is treated as one good trial. For"
" example, \"normal(0, 1)\" effectly equals to a trial with x = 0 which "
"has yielded good result."
msgstr ""

#: ../../Tuner/TpeTuner.rst:109
msgid "``tpe_args.gamma``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:111
msgid "``0.25``"
msgstr ""

#: ../../Tuner/TpeTuner.rst:112
msgid "Controls how many trials are considered \"good\"."
msgstr ""

#: ../../Tuner/TpeTuner.rst:114
msgid "The number is calculated as \"min(gamma * sqrt(N), linear_forgetting)\"."
msgstr ""

